---
editor_options: 
  chunk_output_type: console
---

```{r setup03, include=FALSE}
options(digits = 3)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
theme_set(theme_bw()+theme(text = element_text(size=18)))
```

# Vowels

## Theory

### Recap

* Sound waves can be described as

$$
s(t) = A \times \cos(2\pi ft + \phi)
$$

  * A — amplitude;
  * f — is the fundamental frequency;
  * φ — phase;
  * t — time.
  
* Speech sounds are complex waves
* Fourier transform --- allows to extract components of the complex wave
* [Larynx produce some sound](https://github.com/agricolamz/2024_HSE_m_instrumental_phonetics/blob/master/images/03.01_larynx.mp4?raw=true)
* [Vocal tract filter some frequencies](https://github.com/agricolamz/2024_HSE_m_instrumental_phonetics/blob/master/images/03.02_MRI-speech.mp4?raw=true)

```{r}
knitr::include_graphics("images/01.07-source-filter.png", dpi = 100)
```

### How shape of the vocal tract influences on vowels? Tube model.

Historically, height and backness are impressionistic linguistic terms:

```{r}
knitr::include_graphics("images/03.03_vowel.png", dpi = 300)
```

But we are intersted just in a cardinal points:

```{r}
knitr::include_graphics("images/03.04_cardinal_vowels.png", dpi = 300)
```

```{r}
knitr::include_graphics("images/03.05_faces.png", dpi = 170)
```

If we analyze acoustics we can get something like this:

```{r}
knitr::include_graphics("images/03.06_plots.png", dpi = 200)
```

|    |   i  |   a  |  u  |
|:--:|:----:|:----:|:---:|
| F1 |  300 |  700 | 300 |
| F2 | 2300 | 1400 | 800 |

However, if we analyze real sounds it could be messy:

```{r}
library(phonTools)
data(pb52)
pb52 %>% 
  mutate(vowel = ipa::convert_phonetics(vowel, from = "xsampa", to = "ipa")) %>%
  ggplot(aes(f2, f1, label = vowel, color = vowel))+
  stat_ellipse()+
  geom_text()+
  scale_x_reverse()+
  scale_y_reverse()
```

Tube model, after [@fant60]: vocal tract is a tube or a set of tubes:

```{r}
knitr::include_graphics("images/03.07_tubes.png", dpi = 300)
```

### Wavelength

```{r}
knitr::include_graphics("images/03.08-wavelength.png", dpi = 100)
```

$$c = \frac{\lambda}{T} = \lambda\times f \approx 33400\text{ cm/s}$$

* c  --- speed of sound; 
* λ --- wavelength; 
* f --- sound frequency; 
* T --- period.


Neutral vocal tract in the position for the vowel ə:

```{r}
knitr::include_graphics("images/03.09-shwa-tube.png", dpi = 100)
```


Resonance is a phenomenon in which a vibrating system or external force drives another system to oscillate with greater amplitude at specific frequencies. The lowest natural frequency at which such a tube resonates will have a wavelength (λ) **four times the length of the tube** (L).

$$c = \frac{\lambda}{T} = \lambda\times f \approx 33400\text{ cm/s}$$

The tube also resonates at **odd multiples** of that frequency.

$$F_1 = \frac{c}{\lambda} = \frac{c}{4 \times L} \approx 500 \text{ Hz}$$
$$F_2 = \frac{c}{\lambda} = \frac{c}{\frac{4}{3} \times L} = \frac{3 \times c}{4 L} \approx 1500 \text{ Hz}$$
$$F_3 = \frac{c}{\lambda} = \frac{c}{\frac{4}{5} \times L} = \frac{5 \times c}{4 L} \approx 2500 \text{ Hz}$$
$$F_n = \frac{c}{\lambda} = \frac{c}{\frac{4}{n} \times L} = \frac{n \times c}{4 L} \approx n \times 500 \text{ Hz}$$

Something like this we can expect from animals:

```{r}
knitr::include_graphics("images/03.10-cat-meow.png")
```

<audio controls>
<source src = "cat.wav">
</audio>

When there is a constriction, back tube and constriction form [Helmholtz resonator](https://en.wikipedia.org/wiki/Helmholtz_resonance).

$$f = \frac{c}{2\pi} \times \sqrt{\frac{A}{V\times L}}$$

* A --- the area of the neck; 
* L --- length of the tube; 
* V --- volume of the air in the body.

```{r}
knitr::include_graphics("images/03.11-tubes.png", dpi = 100)
```

### Other models

* Perturbation Theory [Kajiyama 1941, Mrayati et al. 1988]
* Quantal Theory [@stevens72]
* Theory of adaptive dispersion [@lindblom88]

## Vowel formants' normalization

This section is based on [@adank03]. However, see the more detailed overview in [@flynn11].

There are three possible sources of variation in vowel formants measurements [@ladefoged57; @pols73: 1095; @adank03]:

* acoustic variation;
* speaker variation;
   * sociolinguistic;
   * anatomical/physiological variation;
* and measurement error ("residual variance" in [@pols73]).

There are a lot of researchers aimed to reduce speaker-related variation using **acoustic vowel normalization** (e. g. [@gerstman68; @lobanov71; @syrdal86]). However there are some researches that afraid that normalization procedures can reduce interesting for the linguistics information like sociolinguistic/dialectal signal in data [@hindle78; @disner80; @thomas02 174--175].

Human listeners deal seemingly effortlessly with all three possible sources of variation, but the dataset from [@peterson52] shows extrordinary variation:

```{r, fig.width=12}
library(phonTools)
data(pb52)
pb52 %>% 
  mutate(vowel = ipa::convert_phonetics(vowel, from = "xsampa", to = "ipa")) %>%
  ggplot(aes(f2, f1, label = vowel, color = vowel))+
  stat_ellipse(show.legend = FALSE)+
  geom_text(show.legend = FALSE)+
  scale_x_reverse()+
  scale_y_reverse()+
  coord_fixed()+
  labs(caption = "data from [Peterson, Barney 1952]")
```

### Acoustic vowel normalization procedures

There are several classes of vowel normalization procedures:

* formant-based procedures [@gerstman68; @lobanov71; @fant75; @syrdal86; @miller89];
* whole-spectrum procedures [@klein70; @pols73; @bladon81; @bladon82; @klatt82];
* Neural networks [@weenink93; @weenink97].

Formant-based procedures are the most compact (just 2- or 3-dimensional represetations) and comparable crosslinguisticaly.

In [@adank03] author compared 11 methods of vowel normalization:

|    | abb                  | method                                            |
|----|----------------------|---------------------------------------------------|
| 1  | HZ                   | the baseline condition, formants in Hz            |
| 2  | LOG                  | a log-transformation of the frequency scale       |
| 3  | BARK                 | a bark-transformation of the frequency scale      |
| 4  | MEL                  | a mel-transformation of the frequency scale       |
| 5  | ERB                  | an ERB-transformation of the frequency scale      |
| 6  | GERSTMAN             | Gerstman’s (1968) range normalization             |
| 7  | LOBANOV              | Lobanov’s (1971) z-score transformation           |
| 8  | NORDSTRÖM & LINDBLOM | Nordström & Lindblom’s (1975) vocal-tract scaling |
| 9  | CLIH i4              | Nearey’s (1978) individual log-mean procedure     |
| 10 | CLIH s4              | Nearey’s (1978) shared log-mean procedure         |
| 11 | SYRDAL & GOPAL       | Syrdal & Gopal’s (1986) bark-distance model       |
| 12 | MILLER               | Miller’s (1989) formant-ratio model               |

### [@lobanov71] z-score transformation

The idea behind the Lobanov's method is simple z-normalization. Imagine some random distribution:

```{r}
set.seed(42)
fake_data <- tibble(x = rnorm(151, mean = 57, sd = 2))
fake_data %>% 
  ggplot(aes(x))+
  geom_histogram(bins = 15)
```

If we apply the folowing normalization, the distribution form will be the same, however the scale will be unified with mean = 0 and standard deviation = 1:

$$x_{normalized} = \frac{x-\mu}{\sigma}$$


```{r}
fake_data %>% 
  mutate(x = scale(x)) %>% 
  ggplot(aes(x))+
  geom_histogram(bins = 15)
```

```{r, echo=TRUE}
library(phonTools)
data(pb52)
pb52 %>% 
  group_by(speaker) %>% 
  mutate(vowel = ipa::convert_phonetics(vowel, from = "xsampa", to = "ipa"),
         scaled_f1 = scale(f1),
         scaled_f2 = scale(f2)) %>%
  ggplot(aes(scaled_f2, scaled_f1, label = vowel, color = vowel))+
  stat_ellipse()+
  geom_text()+
  scale_x_reverse()+
  scale_y_reverse()
```

You can find implementation of other methods in R package `vowels`.

Try to normalize and visualize data from the dataset Hillenbrand et al. (1995), stored in `h95` variable in the package `phonTools`.

Sometimes it make sense to get back to the formant values:

```{r, echo=TRUE}
pb52 %>% 
  mutate(overall_mean_f1 = mean(f1),
         overall_sd_f1 = sd(f1),
         overall_mean_f2 = mean(f2),
         overall_sd_f2 = sd(f2)) %>% 
  group_by(speaker) %>% 
  mutate(vowel = ipa::convert_phonetics(vowel, from = "xsampa", to = "ipa"),
         sclaed_f1 = scale(f1),
         sclaed_f2 = scale(f2),
         restored_f1 = sclaed_f1*overall_sd_f1+overall_mean_f1,
         restored_f2 = sclaed_f2*overall_sd_f2+overall_mean_f2) %>%
  ggplot(aes(restored_f2, restored_f1, label = vowel, color = vowel))+
  stat_ellipse()+
  geom_text()+
  scale_x_reverse()+
  scale_y_reverse()
```

### `vowels` package

You can find implementation of other methods in R package `vowels`:

```{r, echo=TRUE}
library(vowels)
data(ohiovowels)
vowelplot(norm.lobanov(ohiovowels), color="vowels", label="vowels")
vowelplot(norm.labov(ohiovowels), color="vowels", label="vowels")
vowelplot(norm.nearey(ohiovowels), color="vowels", label="vowels")
vowelplot(norm.wattfabricius(ohiovowels), color="vowels", label="vowels")
```